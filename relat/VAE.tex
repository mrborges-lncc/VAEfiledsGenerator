%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introdution}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Variational auto-encoder\index{variational auto-encoding} (\VAE) model is a stochastic inference and learning algorithm based on variational Bayes (VB) inference proposed by \cite{kingma2014}.
This is a generative technique whose central idea is the development of representations in a low-dimensional latent space that can be mapped back into a realistic-looking image.

\cite{Higgins2016betaVAELB} introduced the \bVAE, a modification of the original \VAE, that introduces an adjustable hyperparameter $\beta$ to balance latent channel capacity and independence constraints with reconstruction accuracy.
They demonstrate that with tuned values of $\beta$ ($\beta>1$) the \bVAE\ outperforms \VAE\ ($\beta=1$).

\cite{Makhzani2015,louizos2017variational,burda2016importance,Zheng2019,vahdat2020}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{\cite{Zhang2022}}

Variational auto-encoder (\VAE) model is a generative network based on variational Bayes (VB) inference proposed by \cite{kingma2014}.

\cite{Zhang2022} proposed a method to reconstruct porous media based on \VAE\ and Fisher information with good quality and efficiency.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Consider the input data $\dataset{X} = \set{\dataeln{x}{i}}{i=1}{\N}$ consisting of $\N$
\textit{i}ndependent and \textit{i}dentically \textit{d}istributed (\iid) samples of the continuous (or discrete) variable $\datael{x}$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[H]
 \centering
 \input{figuras/vae.pdf_t}
\end{figure}

\begin{equation}
 \dataset{Z} = \me{\dataset{Z}} + \desv{\dataset{Z}} \cdot \varepsilon, \quad \mbox{where} \quad \varepsilon \sim \normalf{0}{1}
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The Kullbackâ€“Leibler divergence (also called relative entropy and I-divergence) is a measure of divergence between two distributions \citep{KLD1951,csiszar1975}:

\begin{equation}
 \dkl{\pdf{\va{P}}}{\pdf{\va{Q}}} =
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\cite{Zheng2019} proposed a Fisher autoencoder
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
